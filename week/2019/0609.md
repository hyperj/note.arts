# ARTS - 2019 Week 6-2

20190609~20190615

## Algorithm



## Review

### [Apache Spark Core—Deep Dive—Proper Optimization](https://www.slideshare.net/databricks/apache-spark-coredeep-diveproper-optimization)

Optimizing spark jobs through a true understanding of spark core. 

**Learn** 

- What is a partition? 
- What is the difference between read/shuffle/write partitions? 
- How to increase parallelism and decrease output files? 
- Where does shuffle data go between stages? 
- What is the "right" size for your spark partitions and files? 
- Why does a job slow down with only a few tasks left and never finish? 
- Why doesn't adding nodes decrease my compute time?

### Review



## Tip



## Share

- [Spark 作业耗时分析](../../share/2019/06/spark-job-time-cost-analysis.md)

## Reference

